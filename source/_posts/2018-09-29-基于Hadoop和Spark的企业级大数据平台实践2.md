---
title: 基于Hadoop和Spark的企业级大数据平台实践之二--租户和安全认证 
author: 梅溪
comments: true
layout: post
slug: 
tags : [大数据]
date: 2018-09-29 09:55:00
---




之前在 {% post_link 2017-03-29-基于Hadoop和Spark的企业级大数据平台实践 [基于Hadoop和Spark的企业级大数据平台实践1] %} 中简单介绍了我们数据产品首个版本的最基本架构。本文是该系列第二篇，重点介绍关于大数据平台的多租户和安全建设。





## 租户划分


租户的划分可以有不同的粒度。


- 在云上，一种最简单粗暴的租户划分方式是每用户一个虚拟集群。这也是一种最省心的划分方式。借助于云基础设施的虚拟化能力，创建不同的虚拟集群，可以做到完美的隔离;
- 另一条路是像MaxCompute这样的产品，单一集群要为阿里巴巴集团无数人员提供服务，租户限定到具体project，每project内有非常细粒度的权限控制。


我们产品暂时对于云没有那么强的亲和性（当然后续我们也要拥抱云），而且我们不只是提供最基本的平台组件，而是要基于平台生长出更多大数据能力，所以我们在租户定义上更多参考了MaxCompute的设计。


我们的租户划分有两个维度:

- 计算资源
- 工作空间


<!--more-->

### 计算资源租户划分

计算资源这一层面其实对的就是YARN的资源队列（对YARN不熟悉的可以看我之前的这篇： {% post_link 2015-05-07-YarnLearn.md [YARN-读书笔记] %}），其特点是具有弹性，而且是虚拟概念。我们对每个计算资源租户，提供的是最小用量保证和最大用量限制。也就是说这一层面解决的是针对计算资源(CPU和内存)的多用户竞争，隔离和共享问题。

![计算资源](/images/yarnqueue.png)


### 工作空间租户划分

工作空间可以理解为是一个数据目录，目录中是一系列Hive表等数据资源，以及UDF等作业资源。用户会被加入工作空间，并被划分不同角色，对应不同权限。开发空间是一种和业务贴合更紧的租户，这个层面解决的主要是数据相关资源（包括数据表和作业资源等）多用户访问的隔离和共享。

![工作空间](/images/project.PNG)

我们在实现的时候走了一点弯路，把工作空间设定为计算资源池的从属单位，也就是划分了一级、二级租户。但这样其实是丧失了很多灵活性（最合理的做法，两个维度应该是正交的），建议规格上最好能做到能灵活指派工作空间所提交的作业在特定资源队列运行。




### 大数据长服务的多租户实现


我这里所谓大数据长服务，典型的就是spark的Hive Thrift Server这种应用。它作为Hive SQL的入口，一般是作为yarn-client模式部署起来，对外提供知名的ip+port 供jdbc访问。

在我们的多租户场景，对不同用户，需要能访问不同的Spark Hive Thrift Server实例。那么痛点在于：

- 怎么部署这多个可能占用相同端口的服务实例？
- 对于用户而言，该怎么知道自己该用哪个服务实例？


我们投入了很多精力，比较完美地解决了这个问题。架构如下图所示。


<img src="/images/k8s-sts.png">


- 我们引入了k8s架构，把Spark的应用改造成一系列k8s 服务。通过k8s 解决了部署和scale的问题；
- k8s管到spark应用的driver侧，计算侧即executor继续使用yarn的资源队列；
- 我们实现了作业网关，便于用户自由提交作业，不需要关心具体的服务选择，即租户信息传递和服务映射过程对用户透明。



## 安全认证


早期的Hadoop版本是没有考虑认证的。这样带来的主要问题有：

- 数据不安全，攻击者可以仿冒Namenode，DataNode，盗取或篡改数据；
- 作业不安全，攻击者可以仿冒相关角色去控制计算作业。




Hadoop在1.0.0之后引入了Kerberos，也成为大数据生态的主要安全认证方案。
相关基本原理可以参见这篇简洁清晰的blog： http://idior.cnblogs.com/archive/2006/03/20/354027.html

wiki:  https://en.wikipedia.org/wiki/Kerberos_(protocol)


<img src="/images/Kerberos.svg.png" width="60%" height="60%">





需要特别指出的是，Kerberos本身无论是配置上还是使用上都比较麻烦，也容易出现各种各样的奇怪问题。很多开发者会选择放弃，所以你会发现即使是一些所谓商业版本Hadoop，也没有使用Kerberos认证，甚至近乎裸奔，这样当然是极不安全的。



具体问题有哪些呢？

- 和原有用户系统的联动问题。没有用户系统会天然支持kerberos这个事情；
- krb 配置以及principle/keytab文件的维护成本。这个没办法，要安全，就有额外的负担。这个过程会侵入到所有的访问Hadoop相关服务的业务；
- 大数据作业运行周期可能很长，比如我们的一些长服务。这其中会有ticket失效问题。

下面我们就来具体看一下这些问题。

### 和原有用户系统的联动问题

开源大数据组件为我们提供了非常丰富的功能，我们从中获益良多。可是一旦需要和我们现有的软件系统结合起来，就难免遇到问题。现在引入Kerberos之后，凭空多出principle和keytab的概念，和原有的用户系统如何结合起来呢？

最干净的做法，当然是改造原有的用户系统，把新增信息作为原有用户信息的扩展。可是这条路对我们而言走不通。因为我们的产品定位是对外输出的，大部分场景下无法侵入到原有用户系统的。

我们的路，只能是内置一个用户系统，然后和外部的用户系统保持协同。最终我们选择了[FreeIPA](https://www.freeipa.org/page/Main_Page). 它可以很好地集成Kerberos，LDAP这些系统，保障了我们产品内部所有服务的用户、认证相关功能的自洽。具体部署方案可以参见[这篇博文](https://blog.csdn.net/xuyaqun/article/details/51596018)

目前和外部用户系统的对接主要是采用添加删除消息推送的模式。



### 相关配置/认证信息的管理问题


了解过kerberos协议就会知道，principle是基于用户+位置信息的。这种实现带来了较高的安全级别，但是同时也会导致管理困难（一个较大规模的大数据系统，用户数目和服务数量都很多）。而且加上keytab文件这个东西，往往让用户摸不着头脑。


我们的对策有：

- 在我们的大数据可视化IDE上完成尽可能多的功能，避免用户直接编程对接；
- 封装更多基础服务，比如作业网关，数据传输服务，尽量避免用户直接访问具体Hadoop服务。可以理解为这是“代理”的思路；
- 前两条其实都是保证了很多认证的事情内部做掉，避免直接对用户暴露。在实在无法避免的情况下，我们在可视化的页面为用户提供相关内容展示和下载（注意这个事情本身需要权限控制）。


这些做法极大提高了带Kerberos认证的Hadoop集群（带与不带完全是两种东西……）的易用性，但是本身也是双刃剑：修改量增加了，而且对于熟悉原生系统的用户来说有重新学习的必要（虽然学习成本不高）。


### 长服务ticket失效问题



熟悉Hadoop生态的同学都了解，Hadoop对其工作负载的设计，从一开始就更多地体现为Job，而不是Service。但是实际上我们构建的很多数据子系统，往往都是需要长期提供服务的。比如离线的Hive ThriftServer，或者是Spark Streaming的应用，没有问题的话是要一直不回头地跑下去的。



也没什么好说的，有必要不时relogin一下。说坑也是坑，但是如果了解其原理，也就见怪不怪了。可以看看[这篇提问](https://stackoverflow.com/questions/34616676/should-i-call-ugi-checktgtandreloginfromkeytab-before-every-action-on-hadoop)


除了relogin，renew_time也需要注意一下。我们甚至还踩到了renew_time太大导致timestamp溢出的问题，总之我经常说，kerberos让你有恋爱的感觉。




## 下回预告


下一次，我们的重点放在数据安全上，讲述我们数据平台的数据安全建设思路。











