---
title: SparkSQL Adaptive Execution
author: 梅溪
comments: true
layout: post
slug: 
tags : [大数据,spark]
date: 2018-12-25 09:55:00
---


# SparkSQL Adaptive Execution




Spark SQL 拥有友好的编程接口和卓越的性能，广受大数据处理从业者的欢迎。但是在典型的shuffle场景比如SQL Join场景中，Spark SQL仍然遭遇了不少性能和稳定性问题。

为此，社区和Intel/Baidu的同学先后两次提出了Adaptive Execution的修改议案。本博客将就相关修改做出说明。

两次修改的JIRA:


- https://issues.apache.org/jira/browse/SPARK-9850        2015年提出。
- https://issues.apache.org/jira/browse/SPARK-23128       2018年提出。


由于第一次的修改自从2016年后就没有实质进展，本文以第二次修改提案为准。

本文假设读者对Spark的基本运行原理已经有所了解。

## 当前存在的问题


我们先想一想，除了数据量大之外，Spark SQL处理数据的特点是什么呢？

- 多是第一手数据。毕竟Spark往往是在做ETL，是数据加工的第一站；
- 数据来源丰富。一般是来自于Kafka，HDFS，Hive。也可能来自于HBase和RMDB。
- 迭代计算过程可以很复杂，也可能结合较多的业务相关UDF。


这些特点带来的问题是：

- 数据质量不高，容易有数据倾斜等问题；
- Spark本身不挑数据源，反过来讲，spark是从各种数据源加载数据，本身对于索引，数据分布状况等信息不敏感（虽然也有简单的CBO机制）；
- 数据经过处理后的输出情况可能很复杂，可能数据量比预期的小很多，也可能膨胀很多。

后两点带来的共同问题是：Spark在计算前难以准确估计加工后的数据大小，会给执行计划的优化带来很多困难。也就是说，虽然Spark SQL在map阶段读入数据时，还能够以一种相对还算的平均（大体平均，具体和文件数目大小相关）切分数据的方式划分任务集，但是一旦投入计算后，经历shuffle阶段时，就容易出现一些任务划分不合理或者说是性能上的瓶颈。

解决这些问题需要Spark SQL具备根据计算过程中的数据输出情况做动态调整，这就是AE的"Adaptive"的由来。下面我们就来看AE实际解决的具体问题。

## 问题一：  shuffle 分区不合理


Spark SQL在shuffle过程中的分区数目是通过spark.sql.shuffle.partitions来指定(不熟悉shuffle的同学可以简单理解为是把数据划分为固定个数的hash桶)，这种固定分区数目的做法会导致数据实际规模和任务并行度的不匹配：

- 数据量过大而分区数较少。造成单个任务处理数据量过多，一方面是并行度上不去，导致作业执行时间拉长。另一方面是带来较多的溢写，增加内存压力，容易出现FullGC，OOM等现象；
- 数据量过小而分区数较多。这显然是一种浪费，少数据量投入较多的任务，给调度侧(Spark driver)带来不必要的负担。而且考虑到数据最终落盘，较多分区意味着较多小文件，对HDFS等系统极不友好。



Spark提供了配置方式去调整shuffle分区数目（最新的spark 2.4甚至可以通过sql 语法中的hint 来调整），但是这个调整很难在生产中实际操作，因为很难明确该调成多少，哪怕今天通过反复执行调试好了，明天数据发生变化，情况可能又不一样了。

另外，一个复杂的Spark作业可能涉及非常多的stage，这个shuffle分区数目的调整可能只对某个stage是最优却并非全局最优。


AE提出了一种根据map stage执行后的数据大小和行数信息来自适应调整shuffle 分区数目的思路。


先看配置：

|Property Name |	Default	|Meaning|
|--------------|-----------|--------|
|spark.sql.adaptive.enabled|	false	|When true, enable adaptive query execution.|
|spark.sql.adaptive.minNumPostShufflePartitions	|1	|The minimum number of post-shuffle partitions used in adaptive execution. This can be used to control the minimum parallelism.|
|spark.sql.adaptive.maxNumPostShufflePartitions	|500|	The maximum number of post-shuffle partitions used in adaptive execution. This is also used as the initial shuffle partition number so please set it to an reasonable value.|
|spark.sql.adaptive.shuffle.targetPostShuffleInputSize	|67108864	|The target post-shuffle input size in bytes of a task. By default is 64 MB.|
|spark.sql.adaptive.shuffle.targetPostShuffleRowCount	|20000000	|The target post-shuffle row count of a task. This only takes effect if row count information is collected.|


配置参数很简单，使能后可以利用配置好的分区数，文件大小以及行数来控制shuffle分区数目。


我们看看设计思路。


SPARK-9850 中的做法是首先增加底层API，为DAGScheduler增加submitMapStage()方法，接受代表给定map stage的ShuffleDependency，执行任务并返回一个MapOutputStatistics对象。


``` scala
    def submitMapStage[K, V, C](
        dependency: ShuffleDependency[K, V, C],
        callback: MapOutputStatistics => Unit,
        callSite: CallSite,
        properties: Properties): JobWaiter[MapOutputStatistics] = {
```



单独提交map stage，可以做到搜集到map stage的数据统计信息，便于后续规划stage的任务划分。


另外就是引入了一个结构ExchangeCoordinator用于指导stage之间数据shuffle的方式。

    A coordinator used to determines how we shuffle data between stages generated by Spark SQL.


ExchangeCoordinator 具体使用上是侵入原有的Exchange结构(spark sql的shuffle执行plan)，根据具体情况把数据进行重分布，确保得到合理的分区数量。




- 
SPARK-23128 对这种做法有质疑，提出三个问题:

- 会增加额外的shuffle阶段；
- 太机械。比如3表join，应该是三张表统一协同去exchange，而不是两两结合去exchange。 
- 无法扩充其他功能，比如修改执行计划，处理数据倾斜。


新的AE方案，和原先不同，没有基于比较底层的RDD来实现，而是在SQL 执行计划层面进行优化。增加新的节点类型QueryStage和QueryStageInput，一个执行计划会划分成多个QueryStage

The idea of the new adaptive execution is to divide the stages earlier based on the SQL execution plan instead of the RDD graph. We will introduce new nodes called QueryStage and QueryStageInput. In adaptive execution mode, an execution plan is divided into multiple QueryStages. Each QueryStage is a sub-tree that runs in a single stage. QueryStageInput is the leaf node of a QueryStage and is used to hide its child stage. It gets the result of its child stage and serves it as the input of the QueryStage. A QueryStage knows all its child stages by collecting the QueryStageInputs so it has a global picture of all shuffle dependencies. We adds QueryStage and QueryStageInputs by finding the Exchanges in the plan. Below is an example of joining 3 tables in one stage.





## Goal
目标是:

- DAGScheduler 提供内部API，可以独立提交DAG stage并收集它们的执行结果
- 具备改变下游stage 交互行为的能力。比如改变reduce 任务的数目，或广播一些数据而不是shuffle
- 修改Spark SQL的Planner，可以设置agg的reduce任务数目，选择join策略


## Architecture

基本思路是在决定reduce端策略前先执行map端。

在map端会先产生足够大的分区数目(比如1000)，然后看这些分区输出的大小，决定
使用多少reduce任务，每个reduce任务要取那些分区的数据，或者是否要广播。要实现这些，reduce任务要有能力去获取多个map 输出分区的数据。


举例，比如A，B两表join，各产生1000个输出分区，可以有三种策略:

- shuffle join, 每个reduce 任务从A，B的输出各选取一些输出分区进行计算。driver决定reduce 任务的数目，以及哪些输出分区到特定reduce 任务。
- 把A的输出广播
- 广播A的一些分区


实现分解:

- DAG Scheduler: 允许单独提交map的stage，并收集统计信息
- Shuffle:  允许reduce 任务获取多个map输出分区
- Spark SQL: 实现具体的adaptive query planning
    

### DAG Scheduler

增加DAGScheduler.submitMapStage()方法，接受代表给定map stage的ShuffleDependency，执行任务并返回一个MapOutputStatistics对象。

      def submitMapStage[K, V, C](
          dependency: ShuffleDependency[K, V, C],
          callback: MapOutputStatistics => Unit,
          callSite: CallSite,
          properties: Properties): JobWaiter[MapOutputStatistics] = {


当map stage结束(或者再早一些)，下游stage就可以以相同的ShuffleDependency 提交，并自动纳入DAGScheduler的dependency tracking。



### Shuffle Layer

除了请求多个block之外，还有两个优化会比较有用:

- 允许shuffle读者要求一些block在接收节点做缓存(方便broadcast复用) 
- 优化block manager的shuffle block server，允许一次disk read获取多个分区id




### Spark SQL

增加一种Exchange operator，我们不妨称之为AdaptiveExchange。为它的子operators提交ShuffleMapStages并基于统计信息创建一个ShuffledRDD。

目前只是针对DataFrame的action应用adaptive query execution。



#### 运行时决策

- 设置Reducer数目。设定一个相对大的分区数目和一个合理的单Reducer处理数据大小
- 决定join策略。shuffle join->broadcast join
- 处理数据倾斜。不同分区数据量差异很大，小的则broadcast，大的也不要整个儿shuffle。



#### 修改

##### 查询编译

在查询编译时，当我们发现一个physical operator定义了requiredChildDistribution(表示此operator对它的输入row如何组织有特殊需求)，就添加一个AdaptiveExchange operator.

This logic is different from our
current physical planning logic, which uses EnsureRequirements to add Exchange operators
and tries to avoid add unnecessary Exchange operators.

At
runtime, an AdaptiveExchange operator will determine if to shuffle data based on the
outputPartitioning of its child and its requiredChildDistribution field.



一个例子。

join->agg

In the query plan for adaptive query execution, the AdaptiveExchange operator appearing
before the Join operator has two children and it can plan how to shuffle the data by considering
stats from both table A and table B. The AdaptiveExchange operator appearing before the
Aggregate operator will check if its input rows already satisfy the required distribution. If so, it is
a NoOp operator.

##### Runtime Execution

AdaptiveExchange的 doExecute方法调用后，首先会获得子operator的RDD，并提交ShuffleMapStages；
之后会阻塞住直到这些stages结束。

提交的stage完成之后，AdaptiveExchange搜集统计信息并调用它的planning策略，决定如何创建ShuffledRDD以获取map输出(是reduce阶段的首个RDD)。

最后，outputPartitioning 被更新(比如设置实际使用的分区数目)，并返回创建的ShuffledRDD.

##### Tasks


- 实现AdaptiveExchange operator，并挂到query planner。有feature flag。
- 对不同的operator，在AdaptiveExchange operator中添加不同的planning策略。
- 扩展实现新的可以支持AdaptiveExchange operator的operators
- 设置合理的一些标量(比如每个reduce处理的数据数量)





##  Limitations and Extensions



当前最主要限制是我们需要大量的map输出分区。将来也许可以实现成一个排序后的大文件然后带着分区offset。



我们可能的扩展

- 扩展到streaming
- 应用到core
- 利用更多统计信息，而不至于size
- 更多SQL join类型？



## 参考

https://issues.apache.org/jira/browse/SPARK-9850



https://github.com/Intel-bigdata/spark-adaptive

    